<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Wesley Aptekar-Cassels | Neural Networks</title>
  <meta name="description" content="A few basics about neural networks and perceptrons">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="preload" href="/fonts/Harriet-v2-Text-Regular-latin1.woff2" as="font" type="font/woff2">
  <link rel="preload" href="/fonts/Harriet-v2-Text-Regular-Italic-latin1.woff2" as="font" type="font/woff2">
  <link rel="preload" href="/fonts/Harriet-v2-Text-Bold-latin1.woff2" as="font" type="font/woff2">
  <link rel="preload" href="/fonts/LatoLatin-Regular.woff2" as="font" type="font/woff2">

  <meta property="og:title" content="Neural Networks">
  <meta property="og:type" content="website">
  <meta property="og:url" content="/posts/visualizing-neural-network-learning">
  <meta property="og:description" content="A few basics about neural networks and perceptrons">
  <meta property="og:site_name" content="Wesley Aptekar-Cassels">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:url" content="/posts/visualizing-neural-network-learning">
  <meta name="twitter:title" content="Neural Networks">
  <meta name="twitter:description" content="A few basics about neural networks and perceptrons">

  <link href="/feed.xml" type="application/rss+xml" rel="alternate" title="Wesley Aptekar-Cassels Last 10 blog posts" />

  <link type="text/css" rel="stylesheet" href="/light.css">
  <link type="text/css" rel="stylesheet" href="/dark.css">
</head>

<body>
  <main>
    <nav class="header-nav">
  <a href="/" class="header-logo" title="Wesley Aptekar-Cassels">Wesley Aptekar-Cassels</a>
  <div class="header-links">
    <a href="/feed.xml" target="_blank" title="RSS">
      <div style="width:16px"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><path d="M56 336c31 0 56 25 56 56s-25 56-56 56-56-25-56-56 25-56 56-56zM0 192c140 0 256 116 256 256h-80c0-48-14-94-48-128S48 272 0 272v-80zM0 64c212 0 384 172 384 384h-80c0-171-133-304-304-304V64z"/></svg></div>
    </a>
    <a href="mailto:me@wesleyac.com" target="_blank" title="Email">
      <div style="width:18px"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M422 407c-24 25-52 43-85 55s-69 18-105 18c-35 0-66-6-95-17s-53-26-73-46-36-43-47-71-17-58-17-90 6-62 18-89 29-51 50-71 46-35 74-47c28-11 58-17 90-17 28 0 55 4 81 12s49 20 69 36 36 36 48 60 18 53 18 85c0 24-3 46-10 64s-16 34-27 46-24 22-38 28-29 10-45 10-29-4-39-12-15-17-15-29h-3c-6 10-15 19-28 28s-28 13-46 13c-28 0-49-9-64-27s-23-42-23-71c0-17 3-34 9-50s14-31 24-44 23-23 38-31 31-12 49-12c15 0 27 4 38 10 10 6 18 15 21 24h1l5-24h54l-24 113c-1 6-2 12-3 19s-2 13-2 19c0 7 1 13 4 18s7 7 15 7c16 0 29-9 39-26s16-40 16-68c0-24-4-45-12-64s-20-34-34-47-32-23-52-29-41-9-65-9c-26 0-49 4-70 13s-39 22-54 38-27 34-35 56c-8 21-13 44-13 69 0 26 4 51 13 72s21 39 37 54 35 27 57 35 46 12 72 12c33 0 61-6 85-16s45-25 65-43zM231 188c-10 0-18 2-25 8s-14 13-19 22-8 18-11 28-4 20-4 30c0 5 0 10 1 16 1 5 3 10 6 15s7 8 12 11 11 5 19 5c11 0 20-3 28-8s14-13 19-21 9-16 11-26 3-19 3-27c0-6 0-13-1-19s-4-12-7-17-7-9-12-12-12-5-20-5z"/></svg></div>
    </a>
    <a href="https://github.com/WesleyAC" target="_blank" title="GitHub">
      <div style="width:18px"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M224 32c124 0 224 103 224 230 0 101-64 188-153 218h-4c-8 0-12-7-12-12 0-8 1-31 1-62 0-21-8-36-16-43 50-6 103-25 103-113 0-25-9-46-23-62 2-6 10-29-2-61h-5c-8 0-27 3-57 24-18-5-37-8-56-8s-38 3-56 8c-30-21-49-24-57-24h-5c-12 32-4 55-2 61-14 16-23 37-23 62 0 88 52 107 102 113-6 6-12 16-14 31-6 3-16 6-26 6-13 0-28-5-39-25 0 0-13-22-35-24-2 0-21 0-1 14 0 0 15 8 25 34 0 0 10 33 53 33 7 0 14 0 22-2v39c0 5-3 11-11 11h-4C64 450 0 364 0 262 0 135 100 32 224 32z"/></svg></div>
    </a>
  </div>
</nav>

    <article>
      <header class="article-header">
        <h1>Neural Networks</h1>
        <div class="article-list-date">
          May 22, 2017
        </div>
      </header>

      <div class="article-content">
        <p><em>Note: This accidentally got published before it was completed - this is a slightly modified version from what was originally published</em></p>

<p>I&#39;ve seen a lot of images trying to explain neural networks that look something like this:</p>

<p><img src="../img/nnvis1/deepnet.png" alt="Generic deep neural network. Image credit: Stack Exchange"></p>

<p>But it&#39;s never been entirely clear to me what this is supposed to represent. I know that the circles are neurons, and the lines in between them connections between neurons, but how exactly would one go about implementing this in code, and what is the process to &quot;train&quot; a neural network?</p>

<p>In this post, I&#39;ll go into what exactly that image above is trying to show, and demonstrate how we can use a much simpler network to compute XOR.</p>

<p>First off, what is the point of a neural network? At the simplest level, all a neural network does is takes a list of numbers as an input, and outputs a list of numbers from it. As it turns out, the specific way that we calculate the output is really, really good at finding patterns in large amounts of data. In this post, I&#39;ll show you how to design neural networks to calculate logical functions like AND, OR, and XOR.</p>

<p>Now, let&#39;s get back to that diagram above. In this diagram, each circle is a &quot;neuron&quot;. The job of a neuron is to take an input (the sum of all of the arrows pointing at it), apply an &quot;activation function&quot;, and output the value returned from that activation function to all of the neurons on the layer below it. Each arrow has a &quot;weight&quot; - you multiply the input by this weight to get the output. The process of &quot;training&quot; a neural network is the process of finding the best weights to get it&#39;s results to match your data.</p>

<p>Let&#39;s take a look at a much simpler network. Here&#39;s a network that can calculate the logical AND function:</p>

<p><img src="../img/nnvis1/and_small.png" alt="AND Network"></p>

<p>In this case, we&#39;ll say that our activation function is a step function - if the input to the neuron is greater than 0.5, it will output 1, otherwise it will output 0. Step functions aren&#39;t very useful for doing more complicated things with neural networks, but they&#39;re easy to reason about, so I&#39;ll use them for most of the examples in this post.</p>

<p>If both of our inputs are 1, both of the input neurons are activated. Since both of the input neurons are activated (outputting 1), they both add 0.5 to the input of the output neuron. Because the input to the output neuron is 1, the output neuron is activated. This is correct, since <code>1 AND 1 == 1</code>. If only a single neuron is activated, the input to the output neuron will only be 0.5, so it will not trigger.</p>

<p>It&#39;s pretty trivial to take this kind of network and apply it to OR instead of AND. We just need to change a few weights:</p>

<p><img src="../img/nnvis1/or_small.png" alt="OR Network"></p>

<p>Now if either input neruon is activated, the output will be activated.</p>

<p>Think for a bit about how you would implement XOR with a system like this. As a reminder, here&#39;s the truth table for XOR:</p>

<p><center></p>

<table><thead>
<tr>
<th style="text-align: center">Input 1</th>
<th style="text-align: center">Input 2</th>
<th style="text-align: center">Output</th>
</tr>
</thead><tbody>
<tr>
<td style="text-align: center">1</td>
<td style="text-align: center">1</td>
<td style="text-align: center">0</td>
</tr>
<tr>
<td style="text-align: center">0</td>
<td style="text-align: center">0</td>
<td style="text-align: center">0</td>
</tr>
<tr>
<td style="text-align: center">1</td>
<td style="text-align: center">0</td>
<td style="text-align: center">1</td>
</tr>
<tr>
<td style="text-align: center">0</td>
<td style="text-align: center">1</td>
<td style="text-align: center">1</td>
</tr>
</tbody></table>

<p></center></p>

<p>To make a neural network that can learn XOR, we&#39;ll need to add a new layer. This layer is called a &quot;hidden&quot; layer, since it&#39;s only used in the intermediate calculation, instead of being the input or the output.</p>

<p><img src="../img/nnvis1/xor_small.jpg" alt="XOR Network"></p>

<p>The way that this works is pretty simple - if just one of the inputs is 1, it will go through the hidden neuron and add 1 to the output. However, if both inputs are true, the hidden neuron in the center gets activated, which subtracts two from the output, leaving it at zero.</p>

<p>This is fundamentally how neural networks work. That said, there are a couple things that you&#39;ll need to change to go from this model to a &quot;real&quot; neural network:</p>

<h3>Activation function</h3>

<p>We&#39;ve been using a step function as our activation. This is simple, but can cause problems when we&#39;re trying to do more complicated things. The most commonly used activation is probably the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> or <a href="https://en.wikipedia.org/wiki/ReLU">ReLU</a>. The sigmoid function works well as an activation for a few reasons:</p>

<ul>
<li>It&#39;s continuous and differentiable. This makes training the network with gradient descent possible.</li>
<li>It&#39;s easy to compute.</li>
<li>It&#39;s output is in the range (0,1). This isn&#39;t required, but it&#39;s nice :)</li>
</ul>

<p>However, recently, the ReLU has become more popular for most networks, since it seems to often give better results. However, both have their own benefits and drawbacks.</p>

<h3>Bias</h3>

<p>In our examples so far, we&#39;ve always wanted to have an input of zero result in an output of zero. However, what if this wasn&#39;t the case? The current setup that we have doesn&#39;t allow for a neuron with zero input to output anything other than zero! In order to fix this, we introduce a &quot;bias&quot;. A bias is a neuron that always outputs 1, which is connected to every non-input neuron in the network. By adjusting the weights of the connections to the bias neuron, you can get a non-zero output from a zero input. Even if you want a zero input to result in a zero output, it&#39;s best to add a bias to your network - it will often result in better performance or faster training.</p>

<h3>Training</h3>

<p>A topic that I didn&#39;t get into in this post is how to find the weights for the connections. While manually finding weights works for simple logical functions, it quickly becomes impossible with hundreds or thousands of neurons. Luckily, there are many ways to automatically train neural networks, which I&#39;ll discuss in future posts.</p>

<p><img src="../img/nnvis1/animation.gif" alt="XOR Perceptron Animation"></p>

<h3>Implementation</h3>

<p>While it can sometimes be useful to think of neural networks as a network of neurons, as I&#39;ve shown in this post. However, it&#39;s often better to think about neural networks on the level of layers. This is useful, since each layer can be represented as a matrix of weights. This makes it easy and fast to implement a network, since there are many optimized matrix implementations out there.</p>

      </div>

      <br>
      <!--
ooops, coronavirus happened, guess we're not doing this anymore :(
hopefully someday...
<p>If you're in NYC and want to meet up over lunch/coffee to chat about the future of technology, <a href="mailto:me@wesleyac.com">get in touch with me</a>.</p>
-->

      <br>
    </article>
  </main>
  <script async src="/mathjax/MathJax.js?config=TeX-MML-AM_CHTML"></script> 
</body>
</html>
